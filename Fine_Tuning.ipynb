{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-08-20T08:57:06.210606Z","iopub.execute_input":"2023-08-20T08:57:06.211170Z","iopub.status.idle":"2023-08-20T08:57:06.251794Z","shell.execute_reply.started":"2023-08-20T08:57:06.211121Z","shell.execute_reply":"2023-08-20T08:57:06.250516Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-final-data/new_dataset/Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install language_tool_python","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:06.258051Z","iopub.execute_input":"2023-08-20T08:57:06.260645Z","iopub.status.idle":"2023-08-20T08:57:25.255758Z","shell.execute_reply.started":"2023-08-20T08:57:06.260606Z","shell.execute_reply":"2023-08-20T08:57:25.254450Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting language_tool_python\n  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from language_tool_python) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from language_tool_python) (4.65.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->language_tool_python) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->language_tool_python) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->language_tool_python) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->language_tool_python) (2023.5.7)\nInstalling collected packages: language_tool_python\nSuccessfully installed language_tool_python-2.7.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading Required Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader,Dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nimport spacy\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport language_tool_python\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:25.257872Z","iopub.execute_input":"2023-08-20T08:57:25.258266Z","iopub.status.idle":"2023-08-20T08:57:43.067965Z","shell.execute_reply.started":"2023-08-20T08:57:25.258227Z","shell.execute_reply":"2023-08-20T08:57:43.066790Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reading Data Into Pandas Dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nlp-final-data/new_dataset/Dataset.csv\")\ndf.rename(columns = {\"0\":\"transcript\",\"1\":\"summary\"},inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.071261Z","iopub.execute_input":"2023-08-20T08:57:43.072509Z","iopub.status.idle":"2023-08-20T08:57:43.276511Z","shell.execute_reply.started":"2023-08-20T08:57:43.072464Z","shell.execute_reply":"2023-08-20T08:57:43.275503Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.278051Z","iopub.execute_input":"2023-08-20T08:57:43.278444Z","iopub.status.idle":"2023-08-20T08:57:43.294931Z","shell.execute_reply.started":"2023-08-20T08:57:43.278393Z","shell.execute_reply":"2023-08-20T08:57:43.293736Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                          transcript  \\\n0  My name is Eric, diligent from shifts in Moria...   \n1  Thank you very much.I hope you understand me.I...   \n2  Good afternoon everyone.My name is Yoshimi Cla...   \n3  OK, so this is a tutorial tutorial on the new ...   \n4  Everybody, so my name is Vicki and.For the nex...   \n\n                                             summary  \n0   Eric, a recent graduate from Stanford Univers...  \n1  This text discusses the difficult task of link...  \n2  Yoshimi Clara, the Secretary General of Jay MC...  \n3  This tutorial is about Velebit 5.0, an open so...  \n4  This work looks at the issue of duplicate inst...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>transcript</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My name is Eric, diligent from shifts in Moria...</td>\n      <td>Eric, a recent graduate from Stanford Univers...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Thank you very much.I hope you understand me.I...</td>\n      <td>This text discusses the difficult task of link...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Good afternoon everyone.My name is Yoshimi Cla...</td>\n      <td>Yoshimi Clara, the Secretary General of Jay MC...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>OK, so this is a tutorial tutorial on the new ...</td>\n      <td>This tutorial is about Velebit 5.0, an open so...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Everybody, so my name is Vicki and.For the nex...</td>\n      <td>This work looks at the issue of duplicate inst...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Splitting Data into Train, Test , Validation","metadata":{}},{"cell_type":"code","source":"train_df,test_df = train_test_split(df,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.296484Z","iopub.execute_input":"2023-08-20T08:57:43.296947Z","iopub.status.idle":"2023-08-20T08:57:43.305285Z","shell.execute_reply.started":"2023-08-20T08:57:43.296894Z","shell.execute_reply":"2023-08-20T08:57:43.304173Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train,val = train_test_split(train_df,test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.306602Z","iopub.execute_input":"2023-08-20T08:57:43.307945Z","iopub.status.idle":"2023-08-20T08:57:43.314728Z","shell.execute_reply.started":"2023-08-20T08:57:43.307909Z","shell.execute_reply":"2023-08-20T08:57:43.313515Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Converted the Pandas Dataframe into Dictionary","metadata":{}},{"cell_type":"code","source":"train_data = train.to_dict('records')\nvalid_data = val.to_dict('records')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.316279Z","iopub.execute_input":"2023-08-20T08:57:43.317097Z","iopub.status.idle":"2023-08-20T08:57:43.332753Z","shell.execute_reply.started":"2023-08-20T08:57:43.317059Z","shell.execute_reply":"2023-08-20T08:57:43.331865Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(train_data),len(valid_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.334874Z","iopub.execute_input":"2023-08-20T08:57:43.335955Z","iopub.status.idle":"2023-08-20T08:57:43.345295Z","shell.execute_reply.started":"2023-08-20T08:57:43.335919Z","shell.execute_reply":"2023-08-20T08:57:43.343948Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(1276, 319)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading the T5 Model","metadata":{}},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('t5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-base')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:43.351627Z","iopub.execute_input":"2023-08-20T08:57:43.351952Z","iopub.status.idle":"2023-08-20T08:57:51.678722Z","shell.execute_reply.started":"2023-08-20T08:57:43.351926Z","shell.execute_reply":"2023-08-20T08:57:51.677731Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7755cc5d2164fe98b786537a924a770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c634db5c2ccd48acbe4e1b56beffd504"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8085d91d219c4995b6e5bcc12b4f9898"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ad7b292e0ca479197185e984697a515"}},"metadata":{}}]},{"cell_type":"code","source":"# Freeze all layers except the last 2 layers\nfor param in model.parameters():\n    param.requires_grad = False\nfor param in model.encoder.block[-2:].parameters():\n    param.requires_grad = True\nfor param in model.decoder.block[-2:].parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.680482Z","iopub.execute_input":"2023-08-20T08:57:51.682015Z","iopub.status.idle":"2023-08-20T08:57:51.690928Z","shell.execute_reply.started":"2023-08-20T08:57:51.681977Z","shell.execute_reply":"2023-08-20T08:57:51.689777Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# To convert Train & Validation Dataset into Pytorch Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data[index]\n        input_text = item['transcript']\n        target_text = item['summary']\n        return {'transcript': input_text, 'summary': target_text}","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.692381Z","iopub.execute_input":"2023-08-20T08:57:51.692986Z","iopub.status.idle":"2023-08-20T08:57:51.704083Z","shell.execute_reply.started":"2023-08-20T08:57:51.692949Z","shell.execute_reply":"2023-08-20T08:57:51.703147Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# To Convert the Tokenized Data into Pytorch Dataset","metadata":{}},{"cell_type":"code","source":"class CustomDataset1(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data[index]\n        input_id = item['input_ids']\n        input_mask = item['attention_mask']\n        target_id = item['target_ids']\n        return {'input_ids': input_id, 'attention_mask': input_mask , 'labels': target_id}","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.705375Z","iopub.execute_input":"2023-08-20T08:57:51.706086Z","iopub.status.idle":"2023-08-20T08:57:51.731331Z","shell.execute_reply.started":"2023-08-20T08:57:51.706048Z","shell.execute_reply":"2023-08-20T08:57:51.727868Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Function to Tokenize the Train & Valid Dataset","metadata":{}},{"cell_type":"code","source":"def preprocess_function(data):\n    input_text = data['transcript']\n    target_text = data['summary']\n    # Tokenize the input and target text\n    input_tokens = tokenizer.encode_plus(\n        input_text,\n        max_length=512,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    target_tokens = tokenizer.encode_plus(\n        target_text,\n        max_length=128,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    return {\n        'input_ids': input_tokens['input_ids'].squeeze(),\n        'attention_mask': input_tokens['attention_mask'].squeeze(),\n        'target_ids': target_tokens['input_ids'].squeeze(),\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.733285Z","iopub.execute_input":"2023-08-20T08:57:51.733644Z","iopub.status.idle":"2023-08-20T08:57:51.831472Z","shell.execute_reply.started":"2023-08-20T08:57:51.733612Z","shell.execute_reply":"2023-08-20T08:57:51.830190Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Grammaticality Loss Function","metadata":{}},{"cell_type":"code","source":"tool = language_tool_python.LanguageTool('en-US')\ndef grammaticality_loss_function(output_logits):\n    output_sentences = tokenizer.batch_decode(torch.argmax(output_logits, dim=-1), skip_special_tokens=True)\n    loss = 0.0\n    cnt = 0\n    for output in output_sentences:\n        matches = tool.check(output)\n        num_errors = len(matches)\n        cnt += len(output.split())\n        loss += num_errors\n    loss /= cnt    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.833041Z","iopub.execute_input":"2023-08-20T08:57:51.833492Z","iopub.status.idle":"2023-08-20T08:57:51.893190Z","shell.execute_reply.started":"2023-08-20T08:57:51.833455Z","shell.execute_reply":"2023-08-20T08:57:51.891944Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Customized Trainer which includes compute loss function","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    def get_train_dataloader(self):\n        return DataLoader(\n            self.train_dataset, \n            batch_size=self.args.train_batch_size, \n            collate_fn=self.data_collator, \n            shuffle=True\n        )\n\n    def get_eval_dataloader(self,eval_dataset):\n        return DataLoader(\n            self.eval_dataset, \n            batch_size=self.args.eval_batch_size, \n            collate_fn=self.data_collator\n        )\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs['labels']\n        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['labels'])\n        logits = outputs.logits\n        \n        # compute custom loss\n        grammaticality_loss = 0\n        grammaticality_loss = grammaticality_loss_function(logits.view(-1, self.model.config.vocab_size))\n        \n        # compute cross entropy loss\n        ce_loss_fct = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n        ce_loss = ce_loss_fct(logits.view(-1, model.module.config.vocab_size), labels.view(-1))\n        \n        # combine losses\n        total_loss = grammaticality_loss*0.1 + ce_loss\n        \n        return (total_loss, outputs) if return_outputs else total_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.898233Z","iopub.execute_input":"2023-08-20T08:57:51.898749Z","iopub.status.idle":"2023-08-20T08:57:51.989972Z","shell.execute_reply.started":"2023-08-20T08:57:51.898693Z","shell.execute_reply":"2023-08-20T08:57:51.988619Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Converting the Train Dataset into Pytorch Dataset and then Tokenizing it","metadata":{}},{"cell_type":"code","source":"# Preprocess the data\ntrain_dataset = CustomDataset(train_data)\ntrain_dict_list = list(train_dataset)\ntrain_dict_list = [preprocess_function(example) for example in train_dict_list]\ntrain_dict_list = [{'input_ids': torch.tensor(example['input_ids']),\n                    'attention_mask': torch.tensor(example['attention_mask']),\n                    'target_ids': torch.tensor(example['target_ids'])} for example in train_dict_list]\ntrain_dataset = CustomDataset1(train_dict_list)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:57:51.991576Z","iopub.execute_input":"2023-08-20T08:57:51.991969Z","iopub.status.idle":"2023-08-20T08:58:03.394040Z","shell.execute_reply.started":"2023-08-20T08:57:51.991935Z","shell.execute_reply":"2023-08-20T08:58:03.392830Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/1794968117.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_dict_list = [{'input_ids': torch.tensor(example['input_ids']),\n/tmp/ipykernel_28/1794968117.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  'attention_mask': torch.tensor(example['attention_mask']),\n/tmp/ipykernel_28/1794968117.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  'target_ids': torch.tensor(example['target_ids'])} for example in train_dict_list]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Converting the Validation Dataset into Pytorch Dataset and then Tokenizing it","metadata":{}},{"cell_type":"code","source":"# Preprocess the data\nvalid_dataset = CustomDataset(valid_data)\nvalid_dict_list = list(valid_dataset)\nvalid_dict_list = [preprocess_function(example) for example in valid_dict_list]\nvalid_dict_list = [{'input_ids': torch.tensor(example['input_ids']),\n                    'attention_mask': torch.tensor(example['attention_mask']),\n                    'target_ids': torch.tensor(example['target_ids'])} for example in valid_dict_list]\nvalid_dataset = CustomDataset1(valid_dict_list)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:58:03.395982Z","iopub.execute_input":"2023-08-20T08:58:03.396408Z","iopub.status.idle":"2023-08-20T08:58:06.408088Z","shell.execute_reply.started":"2023-08-20T08:58:03.396371Z","shell.execute_reply":"2023-08-20T08:58:06.407139Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/1384287669.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  valid_dict_list = [{'input_ids': torch.tensor(example['input_ids']),\n/tmp/ipykernel_28/1384287669.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  'attention_mask': torch.tensor(example['attention_mask']),\n/tmp/ipykernel_28/1384287669.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  'target_ids': torch.tensor(example['target_ids'])} for example in valid_dict_list]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Defining Training Arguments","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='model_save',\n    num_train_epochs=10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_strategy=\"epoch\",\n    save_total_limit=5,\n    learning_rate=1e-4,\n    lr_scheduler_type='linear', \n    warmup_steps=0,\n    dataloader_num_workers=4,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:58:06.409708Z","iopub.execute_input":"2023-08-20T08:58:06.410308Z","iopub.status.idle":"2023-08-20T08:58:06.423937Z","shell.execute_reply.started":"2023-08-20T08:58:06.410271Z","shell.execute_reply":"2023-08-20T08:58:06.422707Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:58:06.425873Z","iopub.execute_input":"2023-08-20T08:58:06.426279Z","iopub.status.idle":"2023-08-20T08:58:06.433922Z","shell.execute_reply.started":"2023-08-20T08:58:06.426242Z","shell.execute_reply":"2023-08-20T08:58:06.432797Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset = valid_dataset,\n    data_collator=data_collator,\n    tokenizer = tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:58:06.435502Z","iopub.execute_input":"2023-08-20T08:58:06.436119Z","iopub.status.idle":"2023-08-20T08:58:12.346968Z","shell.execute_reply.started":"2023-08-20T08:58:06.436015Z","shell.execute_reply":"2023-08-20T08:58:12.345896Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ntrainer.model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:58:12.348661Z","iopub.execute_input":"2023-08-20T08:58:12.349074Z","iopub.status.idle":"2023-08-20T08:58:12.372705Z","shell.execute_reply.started":"2023-08-20T08:58:12.349038Z","shell.execute_reply":"2023-08-20T08:58:12.371719Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-20T08:58:12.374174Z","iopub.execute_input":"2023-08-20T08:58:12.374542Z","iopub.status.idle":"2023-08-20T09:39:21.038890Z","shell.execute_reply.started":"2023-08-20T08:58:12.374508Z","shell.execute_reply":"2023-08-20T09:39:21.038001Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230820_085830-m99np2s9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/top_g/huggingface/runs/m99np2s9' target=\"_blank\">breezy-bird-55</a></strong> to <a href='https://wandb.ai/top_g/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/top_g/huggingface' target=\"_blank\">https://wandb.ai/top_g/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/top_g/huggingface/runs/m99np2s9' target=\"_blank\">https://wandb.ai/top_g/huggingface/runs/m99np2s9</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1590/1590 40:10, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.021700</td>\n      <td>2.388150</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.472700</td>\n      <td>2.247518</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.391400</td>\n      <td>2.213656</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.321200</td>\n      <td>2.193710</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.295900</td>\n      <td>2.171991</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.241600</td>\n      <td>2.138334</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.210000</td>\n      <td>2.128771</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.168600</td>\n      <td>2.128590</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.178300</td>\n      <td>2.124144</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.147400</td>\n      <td>2.126884</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1590, training_loss=2.3452552459524862, metrics={'train_runtime': 2468.2715, 'train_samples_per_second': 5.17, 'train_steps_per_second': 0.644, 'total_flos': 7745944367923200.0, 'train_loss': 2.3452552459524862, 'epoch': 9.97})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Saving the best Model depending on Evaluation Loss","metadata":{}},{"cell_type":"code","source":"# Save the best model\ntrainer.save_model(training_args.output_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:39:21.043680Z","iopub.execute_input":"2023-08-20T09:39:21.051423Z","iopub.status.idle":"2023-08-20T09:39:23.235634Z","shell.execute_reply.started":"2023-08-20T09:39:21.051385Z","shell.execute_reply":"2023-08-20T09:39:23.231226Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Trained Model","metadata":{}},{"cell_type":"code","source":"# load the tokenizer\ntrained_tokenizer = T5Tokenizer.from_pretrained('./model_save')\n\n# load the model\ntrained_model = T5ForConditionalGeneration.from_pretrained('./model_save', \n                                                  state_dict=torch.load('./model_save/pytorch_model.bin'))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:39:23.247505Z","iopub.execute_input":"2023-08-20T09:39:23.248001Z","iopub.status.idle":"2023-08-20T09:39:27.403884Z","shell.execute_reply.started":"2023-08-20T09:39:23.247954Z","shell.execute_reply":"2023-08-20T09:39:27.402780Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"trained_model = trained_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:39:27.405225Z","iopub.execute_input":"2023-08-20T09:39:27.405858Z","iopub.status.idle":"2023-08-20T09:39:27.653384Z","shell.execute_reply.started":"2023-08-20T09:39:27.405822Z","shell.execute_reply":"2023-08-20T09:39:27.652020Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Function To Generate Summaries","metadata":{}},{"cell_type":"code","source":"def generate_summary(model,tokenizer,input_text):\n    # Tokenize the input text\n    input_ids = tokenizer.encode(\n        input_text,\n        max_length=512,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    input_ids = input_ids.to(device)\n\n    summary_ids = model.generate(input_ids, num_beams=4, max_length=128, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    \n    return summary","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:39:27.658880Z","iopub.execute_input":"2023-08-20T09:39:27.659362Z","iopub.status.idle":"2023-08-20T09:39:29.164281Z","shell.execute_reply.started":"2023-08-20T09:39:27.659318Z","shell.execute_reply":"2023-08-20T09:39:29.163213Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Generating Summaries for Test Data using Trained Model","metadata":{}},{"cell_type":"code","source":"# Generate the model's summaries for the test data\nreference_summaries = []\ngenerated_summaries = []\nfor i in tqdm(range(test_df.shape[0])):\n    generated_summary = generate_summary(trained_model, trained_tokenizer,test_df.iloc[i][\"transcript\"])\n    generated_summaries.append(generated_summary)\n    reference_summaries.append(test_df.iloc[i][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:39:29.171689Z","iopub.execute_input":"2023-08-20T09:39:29.172452Z","iopub.status.idle":"2023-08-20T09:53:07.041863Z","shell.execute_reply.started":"2023-08-20T09:39:29.172415Z","shell.execute_reply":"2023-08-20T09:53:07.040791Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 399/399 [13:36<00:00,  2.05s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Example 1 :-","metadata":{}},{"cell_type":"markdown","source":"# Reference Summary","metadata":{}},{"cell_type":"code","source":"reference_summaries[10]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:06:59.869439Z","iopub.execute_input":"2023-08-20T10:06:59.869923Z","iopub.status.idle":"2023-08-20T10:06:59.879296Z","shell.execute_reply.started":"2023-08-20T10:06:59.869885Z","shell.execute_reply":"2023-08-20T10:06:59.878286Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'This presentation is about a project called Euclid, in which a consortium of four partners will develop and implement a curriculum to train practitioners in using linked data. The consortium consists of two small-medium enterprises (SMEs) and two academic partners. One of the SMEs, OnToText, is a company that develops repository and RDF store which is graph database used in a number of industry projects. The other SME, See I Research, works closely with the Semantic Technology Institute Association, which brings together 40 institutions worldwide. The academic partners are KIT, an academic institution with a record in the semantic web area and the Open University, the largest online distance learning university. The project seeks to develop living learning materials and an ebook that will be released on iTunes U and other channels. The materials will cover fundamentals of linked data and topics not currently covered by any training curriculum. The project began in May and will go on for two years.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generated Summary","metadata":{}},{"cell_type":"code","source":"generated_summaries[10]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:07:05.593547Z","iopub.execute_input":"2023-08-20T10:07:05.593926Z","iopub.status.idle":"2023-08-20T10:07:05.602353Z","shell.execute_reply.started":"2023-08-20T10:07:05.593894Z","shell.execute_reply":"2023-08-20T10:07:05.601399Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'The consortium will develop and implement a curriculum to train practitioners in using linked data and semantic technologies. The consortium consists of two SMS and two academic partners. The consortium consists of two companies based in Vienna that work closely together with an Association called SGI International which organizes a wide variety of events. The consortium will provide the knowledge and background required in order to develop curriculum training materials that are useful for large audiences. The consortium will also include the Open University, which has over 2000 students and 50,000 students. The consortium will also provide the knowledge and background of the curriculum.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Example 2 :-","metadata":{}},{"cell_type":"markdown","source":"# Reference Summary","metadata":{}},{"cell_type":"code","source":"reference_summaries[6]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:09:54.335761Z","iopub.execute_input":"2023-08-20T10:09:54.336122Z","iopub.status.idle":"2023-08-20T10:09:54.344187Z","shell.execute_reply.started":"2023-08-20T10:09:54.336092Z","shell.execute_reply":"2023-08-20T10:09:54.342999Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'The man is creating a mock up T-shirt on Photoshop. He is using displacement map and image mode grayscale to blur the image and then creating a path around the object. He is also adding adjustment layers to tone down the details of the shirt. He suggests to select a little bit beyond the shirt outlines to avoid having white lines of unused part of the shirt.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generated Summary","metadata":{}},{"cell_type":"code","source":"generated_summaries[6]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:10:00.863027Z","iopub.execute_input":"2023-08-20T10:10:00.863398Z","iopub.status.idle":"2023-08-20T10:10:00.872963Z","shell.execute_reply.started":"2023-08-20T10:10:00.863368Z","shell.execute_reply":"2023-08-20T10:10:00.871772Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'This text is about creating a displacement map with grayscale filter blur and Gaussian blur. The text is about creating a path around the object to create a mockup of the shirt. The text also mentions how to select a little beyond the shirt outlines to create a white line. The text also mentions how to tone down the details using different adjustment layers.'"},"metadata":{}}]},{"cell_type":"code","source":"pip install rouge","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:53:07.096041Z","iopub.execute_input":"2023-08-20T09:53:07.096454Z","iopub.status.idle":"2023-08-20T09:53:19.734239Z","shell.execute_reply.started":"2023-08-20T09:53:07.096421Z","shell.execute_reply":"2023-08-20T09:53:19.733116Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculating Rouge Score for Test Data","metadata":{}},{"cell_type":"code","source":"from rouge import Rouge\n\nrouge = Rouge()\n\n# Calculate ROUGE scores\nscores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n\nprint(scores)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T09:53:19.736772Z","iopub.execute_input":"2023-08-20T09:53:19.737561Z","iopub.status.idle":"2023-08-20T09:53:23.389831Z","shell.execute_reply.started":"2023-08-20T09:53:19.737518Z","shell.execute_reply":"2023-08-20T09:53:23.388621Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"{'rouge-1': {'r': 0.3082220133357383, 'p': 0.48033190241680696, 'f': 0.3655593353838794}, 'rouge-2': {'r': 0.11069361702882946, 'p': 0.17909887451621012, 'f': 0.13220770059350098}, 'rouge-l': {'r': 0.2881786147180402, 'p': 0.45022032772213255, 'f': 0.34208033746040084}}\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install bert_score","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:00:48.356309Z","iopub.execute_input":"2023-08-20T10:00:48.356701Z","iopub.status.idle":"2023-08-20T10:01:00.381567Z","shell.execute_reply.started":"2023-08-20T10:00:48.356668Z","shell.execute_reply":"2023-08-20T10:01:00.380489Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Collecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.0.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.5.3)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.30.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.31.0)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.65.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.16.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2023.5.7)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculating BERTScore for Test Data","metadata":{}},{"cell_type":"code","source":"from bert_score import score\n\n# Calculate BERTScore\npt_score = score(cands=generated_summaries, refs=reference_summaries, lang=\"en\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:01:05.247992Z","iopub.execute_input":"2023-08-20T10:01:05.248598Z","iopub.status.idle":"2023-08-20T10:01:35.791026Z","shell.execute_reply.started":"2023-08-20T10:01:05.248553Z","shell.execute_reply":"2023-08-20T10:01:35.789875Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5e407118ba41a7ad9cc75d60a1e505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842e56e4018c43ecb4c797daf2678a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e1b3ddf5204f1199407b64f60ac027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a37833340404e4997caeeca5e5143a6"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate average precision, recall, and F1 score\navg_precision = pt_score[0].mean().item()\navg_recall = pt_score[1].mean().item()\navg_f1 = pt_score[2].mean().item()\nprint(\"Average Precision:\", avg_precision)\nprint(\"Average Recall:\", avg_recall)\nprint(\"Average F1 Score:\", avg_f1)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T10:01:40.450766Z","iopub.execute_input":"2023-08-20T10:01:40.451153Z","iopub.status.idle":"2023-08-20T10:01:40.469188Z","shell.execute_reply.started":"2023-08-20T10:01:40.451122Z","shell.execute_reply":"2023-08-20T10:01:40.468023Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Average Precision: 0.8707130551338196\nAverage Recall: 0.8627409338951111\nAverage F1 Score: 0.866637110710144\n","output_type":"stream"}]}]}